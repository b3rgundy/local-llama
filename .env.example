# Images
LLAMA_CPP_IMAGE=ghcr.io/ggml-org/llama.cpp:server
OPEN_WEBUI_IMAGE=ghcr.io/open-webui/open-webui:dev
KOKORO_TTS_IMAGE=ghcr.io/remsky/kokoro-fastapi-cpu:latest

# Model config
MODEL_FILENAME=
#MODEL_DIRECTORY=
#CHAT_TEMPLATE=llama2

# LLaMA config (values are for 2022 M2 8GB MacBook Pro)
#CTX_SIZE=512 # size of the prompt context (default: 4096, 0 = loaded from model)
#GPU_LAYERS=2  # number of layers to store in VRAM
#BATCH_SIZE=512 # logical maximum batch size (default: 2048)
#N_PREDICT=128 # number of tokens to predict (default: -1, -1 = infinity)

# Keys
#AUDIO_TTS_OPENAI_API_KEY=
#WEBUI_SECRET_KEY=


# Custom ports configuration
#KOKORO_TTS_PORT=
#LLAMA_PORT=